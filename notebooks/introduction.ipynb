{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow-Based Swap Proposals for Faster Molecular Sampling  \n",
    "*An intuition-first, CPU-friendly tutorial*\n",
    "\n",
    "**Goals**\n",
    "\n",
    "1. *Motivation* — why sampling Boltzmann distributions of molecules is hard.  \n",
    "2. *Parallel Tempering (PT)* quick refresher.  \n",
    "3. *Normalizing Flows (RealNVP)* from first principles, with visualisations.  \n",
    "4. *Toy experiment* on a 2-D Gaussian Mixture Model (GMM):  \n",
    "   * Train a flow between two temperatures.  \n",
    "   * Compare swap acceptance rates: naïve PT vs flow-guided PT.  \n",
    "5. *Bridge* to real peptides (Alanine Dipeptide) and the T-GePT thesis.\n",
    "\n",
    "> **Runtime:** < 10 min on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell purpose: import minimal dependencies and set global settings\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Use CPU throughout for portability\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Helper for nice, reproducible plots\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.grid\": True,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "})\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1  Why is molecular sampling hard?\n",
    "\n",
    "* The Boltzmann density  \n",
    "  $$\n",
    "    \\pi_\\beta(\\mathbf{x}) \\propto e^{-\\beta U(\\mathbf{x})}\n",
    "  $$\n",
    "  has **many deep wells** (metastable states).\n",
    "\n",
    "* Local proposals (e.g. Langevin) **mix slowly** between wells.\n",
    "\n",
    "* **Parallel Tempering (PT)** runs $K$ replicas at temperatures  \n",
    "  $\\beta_1 < \\beta_2 < \\dots < \\beta_K$ and occasionally swaps them.\n",
    "\n",
    "* Unfortunately, a *blind swap* \\\\((x^{(k)},x^{(k+1)}) \\mapsto (x^{(k+1)},x^{(k)})\\\\)  \n",
    "  is only accepted if the two distributions **overlap strongly**.\n",
    "  On tough systems the acceptance may fall below 1 %.\n",
    "\n",
    "### Idea: learn a transport map that *morphs* a hot sample into a cold-like one\n",
    "A *normalizing flow* does exactly that while still allowing us to write down an\n",
    "exact proposal density, so we can keep the Metropolis–Hastings correction and\n",
    "**preserve detailed balance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2  A 2-D GMM toy landscape\n",
    "\n",
    "We first build a simple 5-mode Gaussian mixture.  \n",
    "At low temperature ($T=1$) each mode is narrow; at high temperature ($T=50$) the\n",
    "modes blur and overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell purpose: define a simple GMM class + helper to plot contours / samples\n",
    "class GMM2D:\n",
    "    def __init__(self):\n",
    "        # 5 centres manually chosen for visual clarity\n",
    "        self.loc = torch.tensor([\n",
    "            [-2.5,  0.8],\n",
    "            [-0.7,  1.5],\n",
    "            [ 1.2,  0.3],\n",
    "            [ 0.1, -1.7],\n",
    "            [ 3.0, -1.3]\n",
    "        ], dtype=torch.float32)\n",
    "        # Slightly eccentric covariances\n",
    "        self.scale_tril = torch.tensor([\n",
    "            [[0.40,  0.05], [0.00, 0.45]],\n",
    "            [[0.10,  0.00], [0.00, 0.15]],\n",
    "            [[0.35,  0.00], [0.00, 0.35]],\n",
    "            [[0.25, -0.07], [0.00, 0.18]],\n",
    "            [[0.12,  0.00], [0.00, 0.25]],\n",
    "        ])\n",
    "        self.cat = torch.distributions.Categorical(torch.ones(5) / 5.)\n",
    "        self.comp = torch.distributions.MultivariateNormal(\n",
    "            self.loc, scale_tril=self.scale_tril, validate_args=False\n",
    "        )\n",
    "        self.mixture = torch.distributions.MixtureSameFamily(self.cat, self.comp)\n",
    "\n",
    "    def sample(self, n: int, T: float = 1.0):\n",
    "        if T == 1.0:\n",
    "            return self.mixture.sample((n,))\n",
    "        # high-T = inflate covariance by sqrt(T)\n",
    "        scaled = torch.distributions.MultivariateNormal(\n",
    "            self.loc, scale_tril=self.scale_tril * np.sqrt(T), validate_args=False\n",
    "        )\n",
    "        mixture_hi = torch.distributions.MixtureSameFamily(self.cat, scaled)\n",
    "        return mixture_hi.sample((n,))\n",
    "\n",
    "    def log_prob(self, x: torch.Tensor, T: float = 1.0):\n",
    "        if T == 1.0:\n",
    "            return self.mixture.log_prob(x)\n",
    "        scaled = torch.distributions.MultivariateNormal(\n",
    "            self.loc, scale_tril=self.scale_tril * np.sqrt(T), validate_args=False\n",
    "        )\n",
    "        mixture_hi = torch.distributions.MixtureSameFamily(self.cat, scaled)\n",
    "        return mixture_hi.log_prob(x)\n",
    "\n",
    "def plot_gmm_samples(gmm, T, n=1000, ax=None, title=\"\"):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    s = gmm.sample(n, T).numpy()\n",
    "    ax.scatter(s[:, 0], s[:, 1], s=7, alpha=0.5)\n",
    "    ax.set_title(f\"{title} (T={T})\")\n",
    "    ax.set_xlim(-5, 5); ax.set_ylim(-5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell purpose: visualise the GMM at T=1 and T=50\n",
    "gmm = GMM2D()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "plot_gmm_samples(gmm, T=1.0, n=1500, ax=ax1, title=\"Low temperature\")\n",
    "plot_gmm_samples(gmm, T=50.0, n=1500, ax=ax2, title=\"High temperature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3  Naïve PT baseline\n",
    "\n",
    "We run two replicas:\n",
    "\n",
    "* Replica 0: low-T (β=1)  \n",
    "* Replica 1: high-T (β=50)\n",
    "\n",
    "We alternate **local Langevin moves** and a **blind swap** and record the\n",
    "acceptance probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell purpose: minimal PT with Langevin + blind swap, CPU-only\n",
    "def mala_step(x, log_prob, step_size=0.1):\n",
    "    \"\"\"One Metropolis-adjusted Langevin step (vectorised).\"\"\"\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    lp = log_prob(x)\n",
    "    grad = torch.autograd.grad(lp.sum(), x)[0]\n",
    "    proposal = x + 0.5 * step_size * grad + torch.sqrt(torch.tensor(step_size)) * torch.randn_like(x)\n",
    "    # MH accept\n",
    "    def log_q(x_from, x_to):\n",
    "        # transition density of forward/backward Langevin (ignoring constant)\n",
    "        diff = x_to - (x_from + 0.5 * step_size * grad)\n",
    "        return -0.25 / step_size * (diff ** 2).sum(dim=-1)\n",
    "    lp_prop = log_prob(proposal)\n",
    "    grad_prop = torch.autograd.grad(lp_prop.sum(), proposal)[0]\n",
    "    log_alpha = (lp_prop - lp) + log_q(proposal, x) - log_q(x, proposal)\n",
    "    accept = (torch.log(torch.rand_like(log_alpha)) < log_alpha)\n",
    "    x_new = torch.where(accept.unsqueeze(-1), proposal.detach(), x.detach())\n",
    "    return x_new, accept.float().mean().item()\n",
    "\n",
    "def run_pt_baseline(n_steps=500, swap_interval=10):\n",
    "    # start each replica from its own distribution\n",
    "    x_low = gmm.sample(1, T=1.0)\n",
    "    x_high = gmm.sample(1, T=50.0)\n",
    "    swap_acc = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # local moves\n",
    "        x_low, _ = mala_step(x_low, lambda y: gmm.log_prob(y, T=1.0), step_size=0.05)\n",
    "        x_high, _ = mala_step(x_high, lambda y: gmm.log_prob(y, T=50.0), step_size=0.05)\n",
    "\n",
    "        # swap attempt\n",
    "        if step % swap_interval == 0:\n",
    "            log_alpha = (\n",
    "                gmm.log_prob(x_high, T=1.0) + gmm.log_prob(x_low, T=50.0)\n",
    "                - gmm.log_prob(x_low, T=1.0) - gmm.log_prob(x_high, T=50.0)\n",
    "            )\n",
    "            accept = torch.rand(()) < torch.exp(log_alpha)\n",
    "            if accept:\n",
    "                x_low, x_high = x_high.clone(), x_low.clone()\n",
    "            swap_acc.append(float(accept))\n",
    "    return np.mean(swap_acc)\n",
    "\n",
    "naive_rate = run_pt_baseline()\n",
    "print(f\"Naïve PT swap acceptance ≈ {naive_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4  Normalizing Flows in a Nutshell\n",
    "\n",
    "* A **flow** is an invertible map \\\\(f:\\\\; \\mathbf{z}\\\\to\\mathbf{x}\\\\) with a\n",
    "  tractable Jacobian determinant.\n",
    "\n",
    "* If \\\\( \\mathbf{z}\\sim \\rho(\\mathbf{z}) = \\mathcal N(0,I) \\\\) then  \n",
    "  \\\\[\n",
    "      p(\\mathbf{x}) = \\rho\\!\\bigl(f^{-1}(\\mathbf{x})\\bigr)\n",
    "      \\;\\Bigl|\\det \\partial_{\\mathbf{x}} f^{-1}(\\mathbf{x}) \\Bigr|.\n",
    "  \\\\]\n",
    "\n",
    "### RealNVP coupling layer\n",
    "\n",
    "Keep half the variables fixed, transform the rest with an **affine** shift & log-scale  \n",
    "computed from the passive half.\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    y_a &= x_a \\\\\n",
    "    y_b &= x_b \\odot \\exp\\bigl(s(x_a)\\bigr) + t(x_a)\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "The log-determinant is simply \\\\(\\sum s(x_a)\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell purpose: define a super-minimal RealNVP for 2-D\n",
    "class AffineCoupling(torch.nn.Module):\n",
    "    def __init__(self, mask, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        in_dim = mask.numel()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_dim, in_dim * 2),  # outputs [s, t]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_masked = x * self.mask\n",
    "        s_t = self.net(x_masked)\n",
    "        s, t = s_t.chunk(2, dim=-1)\n",
    "        s = torch.tanh(s) * 2.0             # soft clamp\n",
    "        y = x_masked + (1 - self.mask) * (x * torch.exp(s) + t)\n",
    "        logdet = ((1 - self.mask) * s).sum(-1)\n",
    "        return y, logdet\n",
    "\n",
    "    def inverse(self, y):\n",
    "        y_masked = y * self.mask\n",
    "        s_t = self.net(y_masked)\n",
    "        s, t = s_t.chunk(2, dim=-1)\n",
    "        s = torch.tanh(s) * 2.0\n",
    "        x = y_masked + (1 - self.mask) * ((y - t) * torch.exp(-s))\n",
    "        logdet = -((1 - self.mask) * s).sum(-1)\n",
    "        return x, logdet\n",
    "\n",
    "class RealNVP(torch.nn.Module):\n",
    "    def __init__(self, n_couplings=8, hidden=128):\n",
    "        super().__init__()\n",
    "        masks = [torch.tensor([1.,0.]), torch.tensor([0.,1.])] * (n_couplings//2)\n",
    "        self.blocks = torch.nn.ModuleList([AffineCoupling(m) for m in masks])\n",
    "\n",
    "    def forward(self, x):\n",
    "        logdet = torch.zeros(x.size(0))\n",
    "        z = x\n",
    "        for c in self.blocks:\n",
    "            z, ld = c.forward(z)\n",
    "            logdet += ld\n",
    "            z = z.flip(-1)  # simple permutation\n",
    "        return z, logdet\n",
    "\n",
    "    def inverse(self, z):\n",
    "        logdet = torch.zeros(z.size(0))\n",
    "        x = z\n",
    "        for c in reversed(self.blocks):\n",
    "            x = x.flip(-1)\n",
    "            x, ld = c.inverse(x)\n",
    "            logdet += ld\n",
    "        return x, logdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell purpose: tiny training loop (≤500 steps) to learn 50→1 transport\n",
    "flow = RealNVP(n_couplings=8, hidden=128)\n",
    "optimiser = torch.optim.Adam(flow.parameters(), lr=1e-3)\n",
    "\n",
    "n_steps = 500      # keep tiny for CPU demo\n",
    "batch = 128\n",
    "T_hi = 50.0\n",
    "\n",
    "for step in range(1, n_steps + 1):\n",
    "    # --- Sample high / low batches\n",
    "    x_hi = gmm.sample(batch, T=T_hi)\n",
    "    x_lo = gmm.sample(batch, T=1.0)\n",
    "\n",
    "    # Map high → low direction\n",
    "    y_lo, ld_inv = flow.inverse(x_hi)\n",
    "    loss_hi = -(gmm.log_prob(y_lo, T=1.0) + ld_inv).mean()\n",
    "\n",
    "    # Map low → high direction (symmetry helps training)\n",
    "    y_hi, ld_fwd = flow.forward(x_lo)\n",
    "    loss_lo = -(gmm.log_prob(y_hi, T=T_hi) + ld_fwd).mean()\n",
    "\n",
    "    loss = loss_hi + loss_lo\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"[{step}/{n_steps}] loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell purpose: plot mapped points vs true low-T points\n",
    "with torch.no_grad():\n",
    "    hi_samples = gmm.sample(2000, T=T_hi)\n",
    "    mapped, _ = flow.inverse(hi_samples)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8,4))\n",
    "plot_gmm_samples(gmm, T=1.0, n=0, ax=ax[0], title=\"True low-T modes\")\n",
    "ax[0].scatter(mapped[:,0], mapped[:,1], s=6, alpha=0.5, label=\"mapped\")\n",
    "ax[0].legend()\n",
    "\n",
    "plot_gmm_samples(gmm, T=T_hi, n=2000, ax=ax[1], title=\"Original high-T samples\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell purpose: rerun PT, but with flow-based extreme swap proposal\n",
    "def run_pt_flow(n_steps=500, swap_interval=10):\n",
    "    x_low = gmm.sample(1, T=1.0)\n",
    "    x_high = gmm.sample(1, T=T_hi)\n",
    "    swap_acc = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # local Langevin as before\n",
    "        x_low, _ = mala_step(x_low, lambda y: gmm.log_prob(y, T=1.0), step_size=0.05)\n",
    "        x_high, _ = mala_step(x_high, lambda y: gmm.log_prob(y, T=T_hi), step_size=0.05)\n",
    "\n",
    "        # flow-based swap\n",
    "        if step % swap_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn_like(x_low)\n",
    "                y_high, ld_fwd = flow.forward(x_low)       # propose new high-T\n",
    "                y_low,  ld_inv = flow.inverse(x_high)      # propose new low-T\n",
    "                log_q_fwd = ld_fwd + ld_inv\n",
    "\n",
    "                # reverse proposal density\n",
    "                _, ld_inv_r = flow.inverse(y_high)\n",
    "                _, ld_fwd_r = flow.forward(y_low)\n",
    "                log_q_rev = ld_fwd_r + ld_inv_r\n",
    "\n",
    "            log_alpha = (\n",
    "                gmm.log_prob(y_low,  T=1.0)  + gmm.log_prob(y_high, T=T_hi)\n",
    "                - gmm.log_prob(x_low, T=1.0) - gmm.log_prob(x_high, T=T_hi)\n",
    "                + log_q_rev - log_q_fwd\n",
    "            )\n",
    "            accept = torch.rand(()) < torch.exp(log_alpha)\n",
    "            if accept:\n",
    "                x_low, x_high = y_low.clone(), y_high.clone()\n",
    "            swap_acc.append(float(accept))\n",
    "    return np.mean(swap_acc)\n",
    "\n",
    "flow_rate = run_pt_flow()\n",
    "print(f\"Flow-guided PT swap acceptance ≈ {flow_rate:.4f}\")\n",
    "print(f\"Speed-up ×{flow_rate / max(1e-9, naive_rate):.1f} over naïve.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  What have we learned?\n",
    "\n",
    "* On the toy GMM, a lightweight 2-D RealNVP **raises swap acceptance**\n",
    "  by an order of magnitude without changing local dynamics.\n",
    "\n",
    "* The same principle scales to high-D molecules **if** we:\n",
    "  * design expressive flows (many couplings, conditioning on atom type);\n",
    "  * keep the Metropolis correction to guarantee correctness.\n",
    "\n",
    "### ✈️ Towards Alanine Dipeptide\n",
    "\n",
    "* Replace `GMM2D` by `AldpBoltzmann` (60-D internal coordinates).  \n",
    "* Train a flow on **pairs of temperatures that were bottlenecks** in naïve PT.  \n",
    "* Integrate learned swaps into `ParallelTempering` for genuine sampling gains.\n",
    "\n",
    "See the follow-up notebooks in this series for the molecular case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6  Further Reading\n",
    "\n",
    "* **Normalizing Flows**  \n",
    "  * Dinh et al., *Real NVP* (2016)  \n",
    "  * Papamakarios et al., *Normalizing flows for probabilistic modeling* (2019)\n",
    "\n",
    "* **Replica Exchange / PT**  \n",
    "  * Hukushima & Nemoto, *Exchange Monte Carlo Method* (1996)  \n",
    "  * Earl & Deem, *Parallel tempering: theory, applications, etc.* (2005)\n",
    "\n",
    "* **Flow-based molecular sampling**  \n",
    "  * Noé et al., *Boltzmann Generators* (2019)  \n",
    "  * Doerr et al., *Timewarp* (NeurIPS 2023)\n",
    "\n",
    "Happy sampling! 🎉"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
