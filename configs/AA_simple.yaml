experiment_name: "AA_simple_01"

# Device selection: "auto" | "cpu" | "cuda"
# Use "auto" to prefer GPU if available.
device: "auto"

# Peptide to train on (two-letter code); auto-fills data paths.
peptide_code: AA

# Temperature ladder (must match the PT dataset)
temperatures:
  values: [300.0, 448.6046433448792, 670.82040309906, 1003.1104803085328, 1500.0]

# Adjacent temperature pairs to train
temperature_pairs:
  - [0, 1]

# Data section â€“ paths will be filled automatically from `peptide_code` unless
# explicitly overridden.
data:
  subsample_rate: 10000    # Use much more data (was 100) - 10x more samples
  filter_chirality: false
  center_coordinates: false
  augment_coordinates: false #to be implemented

# Model hyper-parameters
model:
  # Architecture selection: "simple" (PTSwapFlow), "graph" (PTSwapGraphFlow), or "transformer" (PTSwapTransformerFlow)
  architecture: "simple"  # Options: "simple", "graph", "transformer"
  
  # Core flow parameters - INCREASED CAPACITY
  flow_layers: 8      # More coupling layers for complex transformations (was 8)
  hidden_dim: 512     # Much larger hidden layers (was 512)
  
  # Graph-specific parameters (only used when architecture="graph")
  graph:
    atom_vocab_size: 4              # H, C, N, O
    atom_embed_dim: 256             # Much larger atom embeddings (was 128)
    hidden_dim: 1024                # Much larger hidden dimension for message passing (was 512)
    num_mp_layers: 6                # Deeper message passing (was 4)
  
  # Transformer-specific parameters (only used when architecture="transformer")
  transformer:
    atom_vocab_size: 4              # H, C, N, O
    atom_embed_dim: 32              # Dimension of atom type embeddings
    transformer_hidden_dim: 128     # Hidden dimension for transformer layers
    mlp_hidden_layer_dims: [128, 128]  # Hidden layer dimensions for MLPs
    num_transformer_layers: 2       # Number of transformer encoder layers per coupling layer
    n_head: 8                       # Number of attention heads
    dim_feedforward: 2048           # Feedforward dimension in transformer
    rff_encoding_dim: 64            # Random Fourier Features encoding dimension
    rff_scale_mean: 1.0             # Mean of RFF frequency scale distribution
    rff_scale_stddev: 1.0           # Std dev of RFF frequency scale distribution

# Training hyper-parameters - ADJUSTED FOR INCREASED CAPACITY
training:
  batch_size: 32      # Smaller batch due to much larger model (was 64)
  learning_rate: 0.0001  # Lower LR for more careful training (was 0.0001)
  num_epochs: 3000     # More epochs for larger model (was 100)
  val_split: 0.1
  early_stopping_patience: 100  # More patience for larger model (was 50)
  clip_grad_norm: 10.0  # Higher grad clipping for stability (was 5.0)
  warmup_epochs: 10   # Add warmup for stability (was 0)
  min_lr: 0.000005    # Lower minimum LR (was 0.00005)
  lr_factor: 0.5      # Less aggressive reduction (was 0.3)
  lr_patience: 10     # More patience for LR reduction (was 5)

  # Loss component weights (can be scheduled over warmup_epochs)
  nll_start: 1.0    # NLL weight at training start
  nll_end: 1.0      # NLL weight at training end
  acc_start: 0.0    # Acceptance loss weight at start (disabled)
  acc_end: 0.5      # Acceptance loss weight at end (disabled)

# Output directories
output:
  base_dir: "outputs"

# Supervisor-provided system parameters (for AldpBoltzmann)
system:
  temperature: 300.0
  energy_cut: 1.0e8 
  energy_max: 1.0e20 
  n_threads: 64
  transform: cartesian 
  shift_dih: false
  env: implicit 

# Target configuration for evaluation and plotting
target:
  name: dipeptide
  kwargs:
    pdb_path: datasets/pt_dipeptides/AA/ref.pdb
    env: implicit 