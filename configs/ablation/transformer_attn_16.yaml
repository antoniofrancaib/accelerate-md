experiment_name: "transformer_attn_16"

# Multi-peptide mode configuration
mode: multi
peptides:
  train: [AA, AK]
  eval: [AA, AK]

multi_mode:
  batching: uniform

device: "auto"

temperatures:
  values: [300.0, 448.6046433448792, 670.82040309906, 1003.1104803085328, 1500.0]

temperature_pairs:
  - [0, 1]

data:
  subsample_rate: 10000
  filter_chirality: false
  center_coordinates: false
  augment_coordinates: true

model:
  architecture: "transformer"
  flow_layers: 8
  hidden_dim: 512
  
  transformer:
    atom_vocab_size: 4
    atom_embed_dim: 32              # Baseline embedding
    transformer_hidden_dim: 128     # Baseline transformer hidden dim
    mlp_hidden_layer_dims: [128, 128]  # Baseline MLP
    num_transformer_layers: 2       # Baseline number of layers
    n_head: 16                      # ABLATION VARIABLE: 16 attention heads
    dim_feedforward: 2048           # Baseline feedforward
    rff_encoding_dim: 64            # Baseline RFF encoding
    rff_scale_mean: 1.0             # Baseline RFF scale
    rff_scale_stddev: 1.0

training:
  batch_size: 32
  learning_rate: 0.0001
  num_epochs: 50
  val_split: 0.1
  early_stopping_patience: 25
  clip_grad_norm: 5.0
  warmup_epochs: 0
  min_lr: 0.00005
  lr_factor: 0.3
  lr_patience: 2
  nll_start: 1.0
  nll_end: 1.0
  acc_start: 0.0
  acc_end: 0.5

output:
  base_dir: "outputs"

system:
  temperature: 300.0
  energy_cut: 1.0e8 
  energy_max: 1.0e20 
  n_threads: 64
  transform: cartesian 
  shift_dih: false
  env: implicit 

target:
  name: dipeptide
  kwargs:
    env: implicit 